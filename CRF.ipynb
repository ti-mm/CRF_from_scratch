{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_tags: int,\n",
    "        batch_first: bool = True\n",
    "    ):\n",
    "        \"\"\"init parameters of CRF\n",
    "        \n",
    "        Args:\n",
    "            num_tags(int): number of tags\n",
    "            batch_first(bool): \n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.Tensor(num_tags, num_tags))\n",
    "        self.start_transitions = nn.Parameter(torch.Tensor(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.Tensor(num_tags))\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        tags: torch.LongTensor,\n",
    "        mask: torch.ByteTensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"计算给定标签序列的负对数似然\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): _description_\n",
    "            tags (torch.LongTensor): _description_\n",
    "            mask (torch.ByteTensor | None, optional): _description_. Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            torch.tensor: 输入tags的负对数似然\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags)\n",
    "            \n",
    "        if self.batch_fisrt:\n",
    "            emissions = emissions.permute(1, 0, 2)\n",
    "            tags = tags.permute(1, 0)\n",
    "            mask = mask.permute(1, 0)\n",
    "            \n",
    "        score = self._compute_score(emissions, tags, mask)\n",
    "        partition = self._compute_partition(emissions, mask)\n",
    "        \n",
    "        return partition - score\n",
    "    \n",
    "    def _compute_score(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        tags: torch.LongTensor,\n",
    "        mask: torch.ByteTensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): _description_\n",
    "            tags (torch.LongTensor): _description_\n",
    "            mask (torch.ByteTensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = tags.shape\n",
    "        first_tags = tags[0]\n",
    "        \n",
    "        score = self.start_transitions[first_tags]\n",
    "        score += emissions[0, torch.arange(batch_size), first_tags]\n",
    "        \n",
    "        mask = mask.type_as(emissions)\n",
    "        for i in range(1, seq_len):\n",
    "            score += (\n",
    "                self.transitions[tags[i], tags[i + 1]] + \n",
    "                emissions[i, torch.arange(batch_size), tags[i]]\n",
    "            ) * mask[i]\n",
    "            \n",
    "        last_valid_index = mask.long().sum(dim=0) - 1\n",
    "        last_tags = tags[last_valid_index, torch.arange(batch_size)]\n",
    "        score += self.end_transitions[last_tags]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _compute_partition(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        mask: torch.ByteTensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): _description_\n",
    "            mask (torch.ByteTensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = emissions.shape[0]\n",
    "        score = self.start_transitions.unsqueeze(0) + emissions[0]\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "            current_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "            current_score = torch.logsumexp(current_score, dim=1) # shape (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].bool().unsqueeze(1), current_score, score)\n",
    "            \n",
    "        score += self.end_transitions\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        mask: torch.ByteTensor | None = None\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): _description_\n",
    "            mask (torch.ByteTensor | None, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions[:2])\n",
    "        \n",
    "        if self.batch_first:\n",
    "            emissions = emissions.permute(1, 0)\n",
    "            mask = mask.permute(1, 0)\n",
    "        \n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "    \n",
    "    def _viterbi_decode(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        mask: torch.ByteTensor\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (seq_len, batch_size, num_tags)\n",
    "            mask (torch.ByteTensor): (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = mask.shape\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history: list[torch.Tensor] = []\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "            broadcast_emissions = emissions.unsqueeze(1)\n",
    "            current_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "            best_score, indices = torch.max(current_score, dim=1)\n",
    "            score = torch.where(mask[i].bool().unsqueeze(1), best_score, score)\n",
    "            history.append(indices)\n",
    "            \n",
    "        score += self.end_transitions\n",
    "        best_score, indices = torch.max(score, dim=1)\n",
    "        seq_end_tags = mask.long().sum(dim=0) - 1\n",
    "        best_paths: list[list[int]] = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            best_last_tag = indices[i]\n",
    "            this_path = [best_last_tag.item()]\n",
    "            for hist in reversed(history[: seq_end_tags[i]]):\n",
    "                best_last_tag = hist[i][this_path[-1]]\n",
    "                this_path.append(best_last_tag.item())\n",
    "            this_path.reverse()\n",
    "            best_paths.append(this_path)\n",
    "        \n",
    "        return best_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e1dc5a924ff4bd86bb3d39ed4900d7d50546a7249b376e4a4f8488c5324c35e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
